{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca7207b-fe49-4d97-b1c5-ff37dfacb6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: torchvision in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: torch in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pyzbar in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python torchvision torch torchvision matplotlib pytesseract pyzbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e192e22c-e881-4ae7-9b7d-184f7fb4312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "\n",
    "# Specify the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Adjust the path if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b92be23-089c-4ef1-9b07-82978e964790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Product: Size - Height: 463, Width: 618\n",
      "Box Structure: Bad Structure\n",
      "Detected Product: Size - Height: 192, Width: 183\n",
      "Box Structure: Good Structure\n",
      "\n",
      "--- Detection Results ---\n",
      "1. Object Detection:\n",
      "   Object 1: Class ID: 1, Score: 0.99, Position: (21, 1), Size: 618x463\n",
      "   Object 2: Class ID: 72, Score: 0.65, Position: (448, 25), Size: 183x192\n",
      "\n",
      "2. Text Recognition (OCR):\n",
      "   No text detected.\n",
      "\n",
      "3. QR Code and Barcode Detection:\n",
      "   No QR codes or barcodes detected.\n",
      "-------------------------\n",
      "Detected Product: Size - Height: 446, Width: 568\n",
      "Box Structure: Bad Structure\n",
      "Detected Product: Size - Height: 326, Width: 75\n",
      "Box Structure: Bad Structure\n",
      "\n",
      "--- Detection Results ---\n",
      "1. Object Detection:\n",
      "   Object 1: Class ID: 1, Score: 0.99, Position: (52, 28), Size: 568x446\n",
      "   Object 2: Class ID: 32, Score: 0.31, Position: (110, 71), Size: 75x326\n",
      "\n",
      "2. Text Recognition (OCR):\n",
      "   No text detected.\n",
      "\n",
      "3. QR Code and Barcode Detection:\n",
      "   No QR codes or barcodes detected.\n",
      "-------------------------\n",
      "Detected Product: Size - Height: 441, Width: 580\n",
      "Box Structure: Bad Structure\n",
      "Detected Product: Size - Height: 278, Width: 107\n",
      "Box Structure: Good Structure\n",
      "\n",
      "--- Detection Results ---\n",
      "1. Object Detection:\n",
      "   Object 1: Class ID: 1, Score: 1.00, Position: (57, 31), Size: 580x441\n",
      "   Object 2: Class ID: 32, Score: 0.46, Position: (138, 59), Size: 107x278\n",
      "\n",
      "2. Text Recognition (OCR):\n",
      "   No text detected.\n",
      "\n",
      "3. QR Code and Barcode Detection:\n",
      "   No QR codes or barcodes detected.\n",
      "-------------------------\n",
      "Detected Product: Size - Height: 432, Width: 475\n",
      "Box Structure: Bad Structure\n",
      "Detected Product: Size - Height: 267, Width: 140\n",
      "Box Structure: Good Structure\n",
      "Detected Product: Size - Height: 292, Width: 75\n",
      "Box Structure: Good Structure\n",
      "Detected Product: Size - Height: 181, Width: 279\n",
      "Box Structure: Bad Structure\n",
      "Detected Product: Size - Height: 276, Width: 193\n",
      "Box Structure: Good Structure\n",
      "Detected Product: Size - Height: 232, Width: 111\n",
      "Box Structure: Good Structure\n",
      "Detected Product: Size - Height: 354, Width: 240\n",
      "Box Structure: Bad Structure\n",
      "\n",
      "--- Detection Results ---\n",
      "1. Object Detection:\n",
      "   Object 1: Class ID: 1, Score: 0.98, Position: (122, 43), Size: 475x432\n",
      "   Object 2: Class ID: 77, Score: 0.85, Position: (301, 166), Size: 140x267\n",
      "   Object 3: Class ID: 32, Score: 0.47, Position: (110, 101), Size: 75x292\n",
      "   Object 4: Class ID: 1, Score: 0.43, Position: (311, 291), Size: 279x181\n",
      "   Object 5: Class ID: 32, Score: 0.38, Position: (117, 192), Size: 193x276\n",
      "   Object 6: Class ID: 32, Score: 0.38, Position: (248, 227), Size: 111x232\n",
      "   Object 7: Class ID: 1, Score: 0.37, Position: (209, 49), Size: 240x354\n",
      "\n",
      "2. Text Recognition (OCR):\n",
      "   No text detected.\n",
      "\n",
      "3. QR Code and Barcode Detection:\n",
      "   No QR codes or barcodes detected.\n",
      "-------------------------\n",
      "Detected Product: Size - Height: 270, Width: 139\n",
      "Box Structure: Good Structure\n",
      "Detected Product: Size - Height: 428, Width: 470\n",
      "Box Structure: Bad Structure\n",
      "Detected Product: Size - Height: 397, Width: 221\n",
      "Box Structure: Bad Structure\n",
      "Detected Product: Size - Height: 321, Width: 308\n",
      "Box Structure: Bad Structure\n",
      "\n",
      "--- Detection Results ---\n",
      "1. Object Detection:\n",
      "   Object 1: Class ID: 77, Score: 0.94, Position: (302, 169), Size: 139x270\n",
      "   Object 2: Class ID: 1, Score: 0.94, Position: (117, 49), Size: 470x428\n",
      "   Object 3: Class ID: 32, Score: 0.68, Position: (121, 64), Size: 221x397\n",
      "   Object 4: Class ID: 1, Score: 0.49, Position: (116, 149), Size: 308x321\n",
      "\n",
      "2. Text Recognition (OCR):\n",
      "   No text detected.\n",
      "\n",
      "3. QR Code and Barcode Detection:\n",
      "   No QR codes or barcodes detected.\n",
      "-------------------------\n",
      "Detected Product: Size - Height: 427, Width: 640\n",
      "Box Structure: Bad Structure\n",
      "Detected Product: Size - Height: 180, Width: 128\n",
      "Box Structure: Good Structure\n",
      "Detected Product: Size - Height: 300, Width: 252\n",
      "Box Structure: Bad Structure\n",
      "\n",
      "--- Detection Results ---\n",
      "1. Object Detection:\n",
      "   Object 1: Class ID: 1, Score: 0.97, Position: (0, 34), Size: 640x427\n",
      "   Object 2: Class ID: 72, Score: 0.61, Position: (511, 25), Size: 128x180\n",
      "   Object 3: Class ID: 1, Score: 0.42, Position: (380, 159), Size: 252x300\n",
      "\n",
      "2. Text Recognition (OCR):\n",
      "   No text detected.\n",
      "\n",
      "3. QR Code and Barcode Detection:\n",
      "   No QR codes or barcodes detected.\n",
      "-------------------------\n",
      "Detected Product: Size - Height: 449, Width: 596\n",
      "Box Structure: Bad Structure\n",
      "Detected Product: Size - Height: 257, Width: 71\n",
      "Box Structure: Good Structure\n",
      "\n",
      "--- Detection Results ---\n",
      "1. Object Detection:\n",
      "   Object 1: Class ID: 1, Score: 0.99, Position: (43, 23), Size: 596x449\n",
      "   Object 2: Class ID: 32, Score: 0.52, Position: (151, 63), Size: 71x257\n",
      "\n",
      "2. Text Recognition (OCR):\n",
      "   No text detected.\n",
      "\n",
      "3. QR Code and Barcode Detection:\n",
      "   No QR codes or barcodes detected.\n",
      "-------------------------\n",
      "Detected Product: Size - Height: 468, Width: 613\n",
      "Box Structure: Bad Structure\n",
      "Detected Product: Size - Height: 120, Width: 136\n",
      "Box Structure: Good Structure\n",
      "\n",
      "--- Detection Results ---\n",
      "1. Object Detection:\n",
      "   Object 1: Class ID: 1, Score: 0.98, Position: (26, 10), Size: 613x468\n",
      "   Object 2: Class ID: 32, Score: 0.62, Position: (306, 359), Size: 136x120\n",
      "\n",
      "2. Text Recognition (OCR):\n",
      "   No text detected.\n",
      "\n",
      "3. QR Code and Barcode Detection:\n",
      "   No QR codes or barcodes detected.\n",
      "-------------------------\n",
      "Detected Product: Size - Height: 437, Width: 566\n",
      "Box Structure: Bad Structure\n",
      "Detected Product: Size - Height: 198, Width: 180\n",
      "Box Structure: Good Structure\n",
      "\n",
      "--- Detection Results ---\n",
      "1. Object Detection:\n",
      "   Object 1: Class ID: 1, Score: 0.99, Position: (44, 35), Size: 566x437\n",
      "   Object 2: Class ID: 32, Score: 0.34, Position: (76, 277), Size: 180x198\n",
      "\n",
      "2. Text Recognition (OCR):\n",
      "   No text detected.\n",
      "\n",
      "3. QR Code and Barcode Detection:\n",
      "   No QR codes or barcodes detected.\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 172\u001b[0m\n\u001b[0;32m    169\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Run object detection\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m detected_objects \u001b[38;5;241m=\u001b[39m detect_objects(frame, model)\n\u001b[0;32m     95\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m detected_objects\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Process detected objects for sorting\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mdetect_objects\u001b[1;34m(frame, model)\u001b[0m\n\u001b[0;32m     18\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m transform(frame)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No need to track gradients\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(img_tensor)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Filter predictions with a threshold\u001b[39;00m\n\u001b[0;32m     23\u001b[0m detected_objects \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     94\u001b[0m             degen_bb: List[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m boxes[bb_idx]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     95\u001b[0m             torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[0;32m     96\u001b[0m                 \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll bounding boxes should have positive height and width.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Found invalid box \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdegen_bb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for target at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m             )\n\u001b[1;32m--> 101\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(images\u001b[38;5;241m.\u001b[39mtensors)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\models\\detection\\backbone_utils.py:58\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m     57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody(x)\n\u001b[1;32m---> 58\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfpn(x)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\ops\\feature_pyramid_network.py:196\u001b[0m, in \u001b[0;36mFeaturePyramidNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    194\u001b[0m     inner_top_down \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(last_inner, size\u001b[38;5;241m=\u001b[39mfeat_shape, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m     last_inner \u001b[38;5;241m=\u001b[39m inner_lateral \u001b[38;5;241m+\u001b[39m inner_top_down\n\u001b[1;32m--> 196\u001b[0m     results\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_result_from_layer_blocks(last_inner, idx))\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     results, names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_blocks(results, x, names)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\ops\\feature_pyramid_network.py:169\u001b[0m, in \u001b[0;36mFeaturePyramidNetwork.get_result_from_layer_blocks\u001b[1;34m(self, x, idx)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_blocks):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m idx:\n\u001b[1;32m--> 169\u001b[0m         out \u001b[38;5;241m=\u001b[39m module(x)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from pyzbar.pyzbar import decode  # Library for QR code and barcode detection\n",
    "\n",
    "# Load Faster R-CNN with ResNet50\n",
    "def load_faster_rcnn():\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Object Detection using Faster R-CNN\n",
    "def detect_objects(frame, model):\n",
    "    transform = T.Compose([T.ToTensor()])  # Convert the image to a tensor\n",
    "    img_tensor = transform(frame).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        predictions = model(img_tensor)\n",
    "\n",
    "    # Filter predictions with a threshold\n",
    "    detected_objects = []\n",
    "    threshold = 0.3  # Threshold for detection\n",
    "    for i, score in enumerate(predictions[0]['scores']):\n",
    "        if score > threshold:\n",
    "            box = predictions[0]['boxes'][i].cpu().numpy()\n",
    "            detected_objects.append({\n",
    "                \"class_id\": int(predictions[0]['labels'][i].cpu().numpy()),\n",
    "                \"score\": float(score.cpu().numpy()),\n",
    "                \"x\": int(box[0]),\n",
    "                \"y\": int(box[1]),\n",
    "                \"width\": int(box[2] - box[0]),\n",
    "                \"height\": int(box[3] - box[1])\n",
    "            })\n",
    "    return detected_objects\n",
    "\n",
    "# Text Recognition using Tesseract\n",
    "def extract_text(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    text = pytesseract.image_to_string(gray)\n",
    "    return text.strip()\n",
    "\n",
    "# QR Code and Barcode Detection using Pyzbar\n",
    "def extract_qr_barcode(frame):\n",
    "    barcodes = decode(frame)\n",
    "    barcode_data_list = []\n",
    "    for barcode in barcodes:\n",
    "        barcode_data = barcode.data.decode('utf-8')\n",
    "        barcode_type = barcode.type\n",
    "        barcode_data_list.append({\"data\": barcode_data, \"type\": barcode_type, \"rect\": barcode.rect})\n",
    "    return barcode_data_list\n",
    "\n",
    "# Analyze box structure based on size\n",
    "def analyze_box_structure(box):\n",
    "    # Assume some thresholds for perfect box structure\n",
    "    acceptable_width_range = (20, 200)  # Example width range in pixels\n",
    "    acceptable_height_range = (30, 300)  # Example height range in pixels\n",
    "\n",
    "    width = box[\"width\"]\n",
    "    height = box[\"height\"]\n",
    "\n",
    "    if (acceptable_width_range[0] <= width <= acceptable_width_range[1]) and (acceptable_height_range[0] <= height <= acceptable_height_range[1]):\n",
    "        return \"Good Structure\"\n",
    "    else:\n",
    "        return \"Bad Structure\"\n",
    "\n",
    "# Main function to capture video and process frames\n",
    "def main():\n",
    "    model = load_faster_rcnn()\n",
    "    cap = cv2.VideoCapture(0)  # Use the laptop's camera\n",
    "\n",
    "    frame_skip = 2  # Skip frames to reduce processing load\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # Resize frame for better performance\n",
    "        frame = cv2.resize(frame, (640, 480))  # Reducing resolution to 640x480\n",
    "\n",
    "        # Skip frames to reduce load\n",
    "        frame_count += 1\n",
    "        if frame_count % frame_skip != 0:\n",
    "            continue  # Skip this frame\n",
    "\n",
    "        # Create a dictionary to store all results\n",
    "        results = {}\n",
    "\n",
    "        # Run object detection\n",
    "        detected_objects = detect_objects(frame, model)\n",
    "        results[\"detected_objects\"] = detected_objects\n",
    "\n",
    "        # Process detected objects for sorting\n",
    "        for obj in detected_objects:\n",
    "            # Measure size\n",
    "            size_info = {\n",
    "                \"height\": obj[\"height\"],\n",
    "                \"width\": obj[\"width\"]\n",
    "            }\n",
    "\n",
    "            # Analyze box structure\n",
    "            structure_status = analyze_box_structure(obj)\n",
    "\n",
    "            # Log results\n",
    "            print(f\"Detected Product: Size - Height: {size_info['height']}, Width: {size_info['width']}\")\n",
    "            print(f\"Box Structure: {structure_status}\")\n",
    "\n",
    "        # Run OCR\n",
    "        text = extract_text(frame)\n",
    "        results[\"text\"] = text\n",
    "\n",
    "        # Run QR/Barcode detection\n",
    "        qr_barcodes = extract_qr_barcode(frame)\n",
    "        results[\"qr_barcodes\"] = qr_barcodes\n",
    "\n",
    "        # Print the aggregated results for this frame in a readable format\n",
    "        print(\"\\n--- Detection Results ---\")\n",
    "        \n",
    "        # Object Detection Results\n",
    "        print(\"1. Object Detection:\")\n",
    "        if results[\"detected_objects\"]:\n",
    "            for idx, obj in enumerate(results[\"detected_objects\"], start=1):\n",
    "                print(f\"   Object {idx}: Class ID: {obj['class_id']}, Score: {obj['score']:.2f}, \"\n",
    "                      f\"Position: ({obj['x']}, {obj['y']}), Size: {obj['width']}x{obj['height']}\")\n",
    "        else:\n",
    "            print(\"   No objects detected.\")\n",
    "\n",
    "        # Text Recognition Results\n",
    "        print(\"\\n2. Text Recognition (OCR):\")\n",
    "        if results[\"text\"]:\n",
    "            print(f\"   Extracted Text: {results['text']}\")\n",
    "        else:\n",
    "            print(\"   No text detected.\")\n",
    "\n",
    "        # QR/Barcode Detection Results\n",
    "        print(\"\\n3. QR Code and Barcode Detection:\")\n",
    "        if results[\"qr_barcodes\"]:\n",
    "            for idx, qr in enumerate(results[\"qr_barcodes\"], start=1):\n",
    "                print(f\"   {idx}. Type: {qr['type']}, Data: {qr['data']}\")\n",
    "        else:\n",
    "            print(\"   No QR codes or barcodes detected.\")\n",
    "        \n",
    "        print(\"-------------------------\")\n",
    "\n",
    "        # Display results on the frame\n",
    "        # Draw detected objects\n",
    "        for obj in detected_objects:\n",
    "            x, y, w, h = obj[\"x\"], obj[\"y\"], obj[\"width\"], obj[\"height\"]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Draw QR/barcode detection area in red\n",
    "        for qr in qr_barcodes:\n",
    "            x, y, w, h = qr['rect'].left, qr['rect'].top, qr['rect'].width, qr['rect'].height\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)  # Red rectangle\n",
    "            cv2.putText(frame, f\"{qr['type']}: {qr['data']}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "        # Show the processed frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dfa087-406c-47b1-a986-c298bcf3d12a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
